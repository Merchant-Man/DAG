{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fbe9f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import io\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "from botocore.exceptions import ClientError\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import os\n",
    "from datetime import timezone\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8ba6e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file='/Users/aesocia/Documents/BGSI/DEV/dwh-dags/bclconvert_appsessions-2025-08-25.csv'\n",
    "API_BASE_URL = \"https://api.aps4.sh.basespace.illumina.com/v2/runs\"\n",
    "API_TOKEN = \"c8c09c39ab664017997f5c1caf4e49b9\"\n",
    "API_BASE = \"https://api.aps4.sh.basespace.illumina.com/v2\"\n",
    "df_app=pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ca057a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3611b773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-08-26T10:26:57.080+0700\u001b[0m] {\u001b[34m974258736.py:\u001b[0m36} INFO\u001b[0m - üì¶ Page offset=0 | received 25 sessions\u001b[0m\n",
      "[\u001b[34m2025-08-26T10:26:58.085+0700\u001b[0m] {\u001b[34m974258736.py:\u001b[0m36} INFO\u001b[0m - üì¶ Page offset=25 | received 25 sessions\u001b[0m\n",
      "[\u001b[34m2025-08-26T10:26:58.087+0700\u001b[0m] {\u001b[34m974258736.py:\u001b[0m42} INFO\u001b[0m - ‚èπ Hit sessions older than cutoff; stopping.\u001b[0m\n",
      "[\u001b[34m2025-08-26T10:26:58.104+0700\u001b[0m] {\u001b[34m974258736.py:\u001b[0m108} INFO\u001b[0m - ‚úî Final DataFrame shape: (2, 17)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "MAX_ROWS   = 1000   # hard cap of newest rows to collect\n",
    "PAGE_LIMIT = 25     # API hard cap\n",
    "SORT_BY    = \"DateCreated\"\n",
    "SORT_DIR   = \"Desc\"  # newest first\n",
    "\n",
    "# Optional: early stop once DateCreated < this value\n",
    "CURR_DS    = \"2025-08-20\"  # e.g., \"2025-08-25\"\n",
    "\n",
    "# ---------------------------\n",
    "# Run\n",
    "# ---------------------------\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Accept\": \"application/json\",\n",
    "}\n",
    "\n",
    "all_rows = []\n",
    "offset = 0\n",
    "\n",
    "while len(all_rows) < MAX_ROWS:\n",
    "    url = (\n",
    "        f\"{API_BASE}/appsessions\"\n",
    "        f\"?offset={offset}&limit={PAGE_LIMIT}\"\n",
    "        f\"&sortBy={SORT_BY}&sortDir={SORT_DIR}\"\n",
    "    )\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    payload = resp.json() or {}\n",
    "    sessions = payload.get(\"Items\", []) or []\n",
    "\n",
    "    # Optional early stop if sessions are older than CURR_DS\n",
    "    if CURR_DS:\n",
    "        sessions = [s for s in sessions if s.get(\"DateCreated\", \"\") >= CURR_DS]\n",
    "        if not sessions:\n",
    "            break\n",
    "\n",
    "    for session in sessions:\n",
    "        if \"BCLConvert\" not in session.get(\"Name\", \"\"):\n",
    "            continue\n",
    "\n",
    "        session_id = session.get(\"Id\")\n",
    "        if not session_id:\n",
    "            continue\n",
    "\n",
    "        # Fetch session detail\n",
    "        detail_url = f\"{API_BASE}/appsessions/{session_id}\"\n",
    "        dresp = requests.get(detail_url, headers=headers)\n",
    "        if dresp.status_code != 200:\n",
    "            logger.warning(f\"‚ö†Ô∏è Detail fetch failed for {session_id}: {dresp.status_code}\")\n",
    "            continue\n",
    "        detail = dresp.json() or {}\n",
    "\n",
    "        props_items = detail.get(\"Properties\", {}).get(\"Items\", []) or []\n",
    "        properties = {i.get(\"Name\"): i.get(\"Content\") for i in props_items if i.get(\"Name\")}\n",
    "\n",
    "        run_items = []\n",
    "        for i in props_items:\n",
    "            if i.get(\"Name\") == \"Input.Runs\":\n",
    "                run_items = i.get(\"RunItems\", []) or []\n",
    "\n",
    "        for run in run_items:\n",
    "            all_rows.append({\n",
    "                \"RowType\": \"Run\",\n",
    "                \"SessionId\": session_id,\n",
    "                \"SessionName\": detail.get(\"Name\"),\n",
    "                \"DateCreated\": detail.get(\"DateCreated\"),\n",
    "                \"DateModified\": detail.get(\"DateModified\"),\n",
    "                \"ExecutionStatus\": detail.get(\"ExecutionStatus\"),\n",
    "                \"ICA_Link\": detail.get(\"HrefIcaAnalysis\"),\n",
    "                \"ICA_ProjectId\": properties.get(\"ICA.ProjectId\"),\n",
    "                \"WorkflowReference\": properties.get(\"ICA.WorkflowSessionUserReference\"),\n",
    "                \"RunId\": run.get(\"Id\"),\n",
    "                \"RunName\": run.get(\"Name\"),\n",
    "                \"PercentGtQ30\": run.get(\"SequencingStats\", {}).get(\"PercentGtQ30\"),\n",
    "                \"FlowcellBarcode\": run.get(\"FlowcellBarcode\"),\n",
    "                \"ReagentBarcode\": run.get(\"ReagentBarcode\"),\n",
    "                \"Status\": run.get(\"Status\"),\n",
    "                \"ExperimentName\": run.get(\"ExperimentName\"),\n",
    "                \"RunDateCreated\": run.get(\"DateCreated\"),\n",
    "            })\n",
    "\n",
    "            if len(all_rows) >= MAX_ROWS:\n",
    "                logger.info(f\"‚úÖ Reached max_rows={MAX_ROWS}. Stopping.\")\n",
    "                break\n",
    "\n",
    "        if len(all_rows) >= MAX_ROWS:\n",
    "            break\n",
    "\n",
    "    offset += PAGE_LIMIT\n",
    "# ---------------------------\n",
    "# Build DataFrame\n",
    "# ---------------------------\n",
    "df = pd.DataFrame(all_rows, columns=[\n",
    "    \"RowType\", \"SessionId\", \"SessionName\", \"DateCreated\", \"DateModified\",\n",
    "    \"ExecutionStatus\", \"ICA_Link\", \"ICA_ProjectId\", \"WorkflowReference\",\n",
    "    \"RunId\", \"RunName\", \"PercentGtQ30\", \"FlowcellBarcode\", \"ReagentBarcode\",\n",
    "    \"Status\", \"ExperimentName\", \"RunDateCreated\"\n",
    "])\n",
    "\n",
    "logger.info(f\"‚úî Final DataFrame shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "03391245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    246246\n",
       "1    245245\n",
       "Name: RunId, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"RunId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d804963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f4ba6646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-08-26T13:29:24.790+0700\u001b[0m] {\u001b[34m1136230620.py:\u001b[0m144} INFO\u001b[0m - ‚úî DataFrame shape before enrichment: (2, 17)\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:29:25.045+0700\u001b[0m] {\u001b[34m1136230620.py:\u001b[0m175} INFO\u001b[0m - ‚úî Final DataFrame shape: (2, 18)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "def fetch_bclconvert_runs_with_yield(\n",
    "    api_base: str,\n",
    "    api_token: str,\n",
    "    *,\n",
    "    max_rows: int = 1000,\n",
    "    page_limit: int = 25,\n",
    "    sort_by: str = \"DateCreated\",\n",
    "    sort_dir: str = \"Desc\",\n",
    "    curr_ds: str | None = None,\n",
    "    logger: logging.Logger | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch newest BCLConvert Run sessions and enrich them with total_flowcell_yield_gbp.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    api_base : str\n",
    "        Base URL for the AppSessions API (e.g., https://api.example.com)\n",
    "    api_token : str\n",
    "        API token for AppSessions\n",
    "    stats_base : str\n",
    "        Base URL for sequencing stats API (e.g., https://api.example.com/runs)\n",
    "    max_rows : int\n",
    "        Hard cap of newest rows to collect\n",
    "    page_limit : int\n",
    "        API page size (server hard cap is 25)\n",
    "    sort_by : str\n",
    "        Field to sort by (default \"DateCreated\")\n",
    "    sort_dir : str\n",
    "        Sort direction (default \"Desc\" = newest first)\n",
    "    curr_ds : str | None\n",
    "        Optional cutoff, only include sessions where DateCreated >= curr_ds\n",
    "    logger : logging.Logger | None\n",
    "        Optional logger; if None, a default logger is created.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame of Run rows, including total_flowcell_yield_gbp column.\n",
    "    \"\"\"\n",
    "\n",
    "    if logger is None:\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "            stream=sys.stdout,\n",
    "        )\n",
    "        logger = logging.getLogger(\"bclconvert-fetch\")\n",
    "\n",
    "    all_rows = []\n",
    "    offset = 0\n",
    "\n",
    "    sessions_headers = {\n",
    "        \"Authorization\": f\"Bearer {api_token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    while len(all_rows) < max_rows:\n",
    "        url = (\n",
    "            f\"{api_base}/appsessions\"\n",
    "            f\"?offset={offset}&limit={page_limit}\"\n",
    "            f\"&sortBy={sort_by}&sortDir={sort_dir}\"\n",
    "        )\n",
    "        resp = requests.get(url, headers=sessions_headers)\n",
    "        resp.raise_for_status()\n",
    "        payload = resp.json() or {}\n",
    "        sessions = payload.get(\"Items\", []) or []\n",
    "\n",
    "        if not sessions:\n",
    "            break\n",
    "\n",
    "        # Optional cutoff\n",
    "        if curr_ds:\n",
    "            sessions = [s for s in sessions if s.get(\"DateModified\", \"\") >= curr_ds]\n",
    "            if not sessions:\n",
    "                break\n",
    "\n",
    "        for session in sessions:\n",
    "            if \"BCLConvert\" not in session.get(\"Name\", \"\"):\n",
    "                continue\n",
    "\n",
    "            session_id = session.get(\"Id\")\n",
    "            if not session_id:\n",
    "                continue\n",
    "\n",
    "            # fetch detail\n",
    "            detail_url = f\"{api_base}/appsessions/{session_id}\"\n",
    "            dresp = requests.get(detail_url, headers=sessions_headers)\n",
    "            if dresp.status_code != 200:\n",
    "                logger.warning(f\"‚ö†Ô∏è Detail fetch failed for {session_id}: {dresp.status_code}\")\n",
    "                continue\n",
    "            detail = dresp.json() or {}\n",
    "\n",
    "            props_items = detail.get(\"Properties\", {}).get(\"Items\", []) or []\n",
    "            properties = {i.get(\"Name\"): i.get(\"Content\") for i in props_items if i.get(\"Name\")}\n",
    "\n",
    "            run_items = []\n",
    "            for i in props_items:\n",
    "                if i.get(\"Name\") == \"Input.Runs\":\n",
    "                    run_items = i.get(\"RunItems\", []) or []\n",
    "\n",
    "            for run in run_items:\n",
    "                all_rows.append({\n",
    "                    \"row_type\": \"Run\",\n",
    "                    \"session_id\": session_id,\n",
    "                    \"session_name\": detail.get(\"Name\"),\n",
    "                    \"date_created\": detail.get(\"DateCreated\"),\n",
    "                    \"date_modified\": detail.get(\"DateModified\"),\n",
    "                    \"execution_status\": detail.get(\"ExecutionStatus\"),\n",
    "                    \"ica_link\": detail.get(\"HrefIcaAnalysis\"),\n",
    "                    \"ica_project_id\": properties.get(\"ICA.ProjectId\"),\n",
    "                    \"workflow_reference\": properties.get(\"ICA.WorkflowSessionUserReference\"),\n",
    "                    \"run_id\": run.get(\"Id\"),\n",
    "                    \"run_name\": run.get(\"Name\"),\n",
    "                    \"percent_gt_q30\": run.get(\"SequencingStats\", {}).get(\"PercentGtQ30\"),\n",
    "                    \"flowcell_barcode\": run.get(\"FlowcellBarcode\"),\n",
    "                    \"reagent_barcode\": run.get(\"ReagentBarcode\"),\n",
    "                    \"status\": run.get(\"Status\"),\n",
    "                    \"experiment_name\": run.get(\"ExperimentName\"),\n",
    "                    \"run_date_created\": run.get(\"DateCreated\"),\n",
    "                })\n",
    "\n",
    "                if len(all_rows) >= max_rows:\n",
    "                    break\n",
    "            if len(all_rows) >= max_rows:\n",
    "                break\n",
    "\n",
    "        offset += page_limit\n",
    "\n",
    "    # ---- build DataFrame ----\n",
    "    df = pd.DataFrame(all_rows, columns=[\n",
    "        \"row_type\", \"session_id\", \"session_name\", \"date_created\", \"date_modified\",\n",
    "        \"execution_status\", \"ica_link\", \"ica_project_id\", \"workflow_reference\",\n",
    "        \"run_id\", \"run_name\", \"percent_gt_q30\", \"flowcell_barcode\", \"reagent_barcode\",\n",
    "        \"status\", \"experiment_name\", \"run_date_created\"\n",
    "    ])\n",
    "\n",
    "    logger.info(f\"‚úî DataFrame shape before enrichment: {df.shape}\")\n",
    "\n",
    "    # ---- enrich with sequencing stats ----\n",
    "    run_rows = df[df[\"row_type\"] == \"Run\"]\n",
    "\n",
    "    for _, row in run_rows.iterrows():\n",
    "        run_id = row.get(\"run_id\")\n",
    "        if not run_id or run_id.lower() == \"nan\":\n",
    "            continue\n",
    "\n",
    "        api_url = f\"{api_base}/runs/{run_id}/sequencingstats\"\n",
    "        stats_headers = {\n",
    "            \"x-access-token\": api_token,\n",
    "            \"Accept\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(api_url, headers=stats_headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            total_yield = data.get(\"YieldTotal\")\n",
    "\n",
    "            if total_yield is not None:\n",
    "                df.loc[\n",
    "                    (df[\"row_type\"] == \"Run\") & (df[\"run_id\"] == run_id),\n",
    "                    \"total_flowcell_yield_Gbp\"\n",
    "                ] = total_yield\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"‚ö†Ô∏è Failed fetching stats for {run_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    logger.info(f\"‚úî Final DataFrame shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "df = fetch_bclconvert_runs_with_yield(\n",
    "    api_base=API_BASE,\n",
    "    api_token=API_TOKEN,\n",
    "    curr_ds=\"2025-08-20\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2b353b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_type</th>\n",
       "      <th>session_id</th>\n",
       "      <th>session_name</th>\n",
       "      <th>date_created</th>\n",
       "      <th>date_modified</th>\n",
       "      <th>execution_status</th>\n",
       "      <th>ica_link</th>\n",
       "      <th>ica_project_id</th>\n",
       "      <th>workflow_reference</th>\n",
       "      <th>run_id</th>\n",
       "      <th>run_name</th>\n",
       "      <th>percent_gt_q30</th>\n",
       "      <th>flowcell_barcode</th>\n",
       "      <th>reagent_barcode</th>\n",
       "      <th>status</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>run_date_created</th>\n",
       "      <th>total_flowcell_yield_Gbp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Run</td>\n",
       "      <td>259259</td>\n",
       "      <td>BCLConvert 08/22/2025 23:57:04Z</td>\n",
       "      <td>2025-08-22T23:57:05.0000000Z</td>\n",
       "      <td>2025-08-23T02:24:32.0000000Z</td>\n",
       "      <td>Complete</td>\n",
       "      <td>https://ica.illumina.com/ica/link/project/7feb...</td>\n",
       "      <td>7feb6619-714b-48f7-a7fd-75ad264f9c55</td>\n",
       "      <td>ws_LP2508211-P1_b823b3</td>\n",
       "      <td>246246</td>\n",
       "      <td>250821_A01856_0274_BHLMJMDSXF</td>\n",
       "      <td>89.5419</td>\n",
       "      <td>HLMJMDSXF</td>\n",
       "      <td>NV2432571-RGSBS</td>\n",
       "      <td>Complete</td>\n",
       "      <td>LP2508211-P1</td>\n",
       "      <td>2025-08-21T06:23:35.0000000Z</td>\n",
       "      <td>3650.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run</td>\n",
       "      <td>256256</td>\n",
       "      <td>BCLConvert 08/21/2025 01:50:00Z</td>\n",
       "      <td>2025-08-21T01:50:00.0000000Z</td>\n",
       "      <td>2025-08-21T04:08:42.0000000Z</td>\n",
       "      <td>Complete</td>\n",
       "      <td>https://ica.illumina.com/ica/link/project/7feb...</td>\n",
       "      <td>7feb6619-714b-48f7-a7fd-75ad264f9c55</td>\n",
       "      <td>ws_LP2508131-P2_cd272a</td>\n",
       "      <td>245245</td>\n",
       "      <td>250819_A01856_0273_BHKGCNDSXF</td>\n",
       "      <td>89.3399</td>\n",
       "      <td>HKGCNDSXF</td>\n",
       "      <td>NV4421056-RGSBS</td>\n",
       "      <td>Complete</td>\n",
       "      <td>LP2508131-P2</td>\n",
       "      <td>2025-08-19T08:17:14.0000000Z</td>\n",
       "      <td>3545.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  row_type session_id                     session_name  \\\n",
       "0      Run     259259  BCLConvert 08/22/2025 23:57:04Z   \n",
       "1      Run     256256  BCLConvert 08/21/2025 01:50:00Z   \n",
       "\n",
       "                   date_created                 date_modified  \\\n",
       "0  2025-08-22T23:57:05.0000000Z  2025-08-23T02:24:32.0000000Z   \n",
       "1  2025-08-21T01:50:00.0000000Z  2025-08-21T04:08:42.0000000Z   \n",
       "\n",
       "  execution_status                                           ica_link  \\\n",
       "0         Complete  https://ica.illumina.com/ica/link/project/7feb...   \n",
       "1         Complete  https://ica.illumina.com/ica/link/project/7feb...   \n",
       "\n",
       "                         ica_project_id      workflow_reference  run_id  \\\n",
       "0  7feb6619-714b-48f7-a7fd-75ad264f9c55  ws_LP2508211-P1_b823b3  246246   \n",
       "1  7feb6619-714b-48f7-a7fd-75ad264f9c55  ws_LP2508131-P2_cd272a  245245   \n",
       "\n",
       "                        run_name  percent_gt_q30 flowcell_barcode  \\\n",
       "0  250821_A01856_0274_BHLMJMDSXF         89.5419        HLMJMDSXF   \n",
       "1  250819_A01856_0273_BHKGCNDSXF         89.3399        HKGCNDSXF   \n",
       "\n",
       "   reagent_barcode    status experiment_name              run_date_created  \\\n",
       "0  NV2432571-RGSBS  Complete    LP2508211-P1  2025-08-21T06:23:35.0000000Z   \n",
       "1  NV4421056-RGSBS  Complete    LP2508131-P2  2025-08-19T08:17:14.0000000Z   \n",
       "\n",
       "   total_flowcell_yield_Gbp  \n",
       "0                   3650.02  \n",
       "1                   3545.24  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc73e874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "def fetch_bclconvert_runs_with_yield(\n",
    "    api_base: str,\n",
    "    api_token: str,\n",
    "    *,\n",
    "    max_rows: int = 1000,\n",
    "    page_limit: int = 25,\n",
    "    sort_by: str = \"DateCreated\",\n",
    "    sort_dir: str = \"Desc\",\n",
    "    curr_ds: str | None = None,\n",
    "    logger: logging.Logger | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch newest BCLConvert Run sessions and enrich them with total_flowcell_yield_gbp.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    api_base : str\n",
    "        Base URL for the AppSessions API (e.g., https://api.example.com)\n",
    "    api_token : str\n",
    "        API token for AppSessions\n",
    "    stats_base : str\n",
    "        Base URL for sequencing stats API (e.g., https://api.example.com/runs)\n",
    "    max_rows : int\n",
    "        Hard cap of newest rows to collect\n",
    "    page_limit : int\n",
    "        API page size (server hard cap is 25)\n",
    "    sort_by : str\n",
    "        Field to sort by (default \"DateCreated\")\n",
    "    sort_dir : str\n",
    "        Sort direction (default \"Desc\" = newest first)\n",
    "    curr_ds : str | None\n",
    "        Optional cutoff, only include sessions where DateCreated >= curr_ds\n",
    "    logger : logging.Logger | None\n",
    "        Optional logger; if None, a default logger is created.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame of Run rows, including total_flowcell_yield_gbp column.\n",
    "    \"\"\"\n",
    "\n",
    "    if logger is None:\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "            stream=sys.stdout,\n",
    "        )\n",
    "        logger = logging.getLogger(\"bclconvert-fetch\")\n",
    "\n",
    "    all_rows = []\n",
    "    offset = 0\n",
    "\n",
    "    sessions_headers = {\n",
    "        \"Authorization\": f\"Bearer {api_token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    while len(all_rows) < max_rows:\n",
    "        url = (\n",
    "            f\"{api_base}/appsessions\"\n",
    "            f\"?offset={offset}&limit={page_limit}\"\n",
    "            f\"&sortBy={sort_by}&sortDir={sort_dir}\"\n",
    "        )\n",
    "        resp = requests.get(url, headers=sessions_headers)\n",
    "        resp.raise_for_status()\n",
    "        payload = resp.json() or {}\n",
    "        sessions = payload.get(\"Items\", []) or []\n",
    "\n",
    "        if not sessions:\n",
    "            break\n",
    "\n",
    "        # Optional cutoff\n",
    "        if curr_ds:\n",
    "            sessions = [s for s in sessions if s.get(\"DateModified\", \"\") >= curr_ds]\n",
    "            if not sessions:\n",
    "                break\n",
    "\n",
    "        for session in sessions:\n",
    "            if \"BCLConvert\" not in session.get(\"Name\", \"\"):\n",
    "                continue\n",
    "\n",
    "            session_id = session.get(\"Id\")\n",
    "            if not session_id:\n",
    "                continue\n",
    "\n",
    "            # fetch detail\n",
    "            detail_url = f\"{api_base}/appsessions/{session_id}\"\n",
    "            dresp = requests.get(detail_url, headers=sessions_headers)\n",
    "            if dresp.status_code != 200:\n",
    "                logger.warning(f\"‚ö†Ô∏è Detail fetch failed for {session_id}: {dresp.status_code}\")\n",
    "                continue\n",
    "            detail = dresp.json() or {}\n",
    "\n",
    "            props_items = detail.get(\"Properties\", {}).get(\"Items\", []) or []\n",
    "            properties = {i.get(\"Name\"): i.get(\"Content\") for i in props_items if i.get(\"Name\")}\n",
    "\n",
    "            run_items = []\n",
    "            for i in props_items:\n",
    "                if i.get(\"Name\") == \"Input.Runs\":\n",
    "                    run_items = i.get(\"RunItems\", []) or []\n",
    "\n",
    "            for run in run_items:\n",
    "                all_rows.append({\n",
    "                    \"row_type\": \"Run\",\n",
    "                    \"session_id\": session_id,\n",
    "                    \"session_name\": detail.get(\"Name\"),\n",
    "                    \"date_created\": detail.get(\"DateCreated\"),\n",
    "                    \"date_modified\": detail.get(\"DateModified\"),\n",
    "                    \"execution_status\": detail.get(\"ExecutionStatus\"),\n",
    "                    \"ica_link\": detail.get(\"HrefIcaAnalysis\"),\n",
    "                    \"ica_project_id\": properties.get(\"ICA.ProjectId\"),\n",
    "                    \"workflow_reference\": properties.get(\"ICA.WorkflowSessionUserReference\"),\n",
    "                    \"run_id\": run.get(\"Id\"),\n",
    "                    \"run_name\": run.get(\"Name\"),\n",
    "                    \"percent_gt_q30\": run.get(\"SequencingStats\", {}).get(\"PercentGtQ30\"),\n",
    "                    \"flowcell_barcode\": run.get(\"FlowcellBarcode\"),\n",
    "                    \"reagent_barcode\": run.get(\"ReagentBarcode\"),\n",
    "                    \"status\": run.get(\"Status\"),\n",
    "                    \"experiment_name\": run.get(\"ExperimentName\"),\n",
    "                    \"run_date_created\": run.get(\"DateCreated\"),\n",
    "                })\n",
    "\n",
    "                if len(all_rows) >= max_rows:\n",
    "                    break\n",
    "            if len(all_rows) >= max_rows:\n",
    "                break\n",
    "\n",
    "        offset += page_limit\n",
    "\n",
    "    # ---- build DataFrame ----\n",
    "    df = pd.DataFrame(all_rows, columns=[\n",
    "        \"row_type\", \"session_id\", \"session_name\", \"date_created\", \"date_modified\",\n",
    "        \"execution_status\", \"ica_link\", \"ica_project_id\", \"workflow_reference\",\n",
    "        \"run_id\", \"run_name\", \"percent_gt_q30\", \"flowcell_barcode\", \"reagent_barcode\",\n",
    "        \"status\", \"experiment_name\", \"run_date_created\"\n",
    "    ])\n",
    "\n",
    "    logger.info(f\"‚úî DataFrame shape before enrichment: {df.shape}\")\n",
    "\n",
    "    # ---- enrich with sequencing stats ----\n",
    "    run_rows = df[df[\"row_type\"] == \"Run\"]\n",
    "\n",
    "    for _, row in run_rows.iterrows():\n",
    "        run_id = row.get(\"run_id\")\n",
    "        if not run_id or run_id.lower() == \"nan\":\n",
    "            continue\n",
    "\n",
    "        api_url = f\"{api_base}/runs/{run_id}/sequencingstats\"\n",
    "        stats_headers = {\n",
    "            \"x-access-token\": api_token,\n",
    "            \"Accept\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(api_url, headers=stats_headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            total_yield = data.get(\"YieldTotal\")\n",
    "\n",
    "            if total_yield is not None:\n",
    "                df.loc[\n",
    "                    (df[\"row_type\"] == \"Run\") & (df[\"run_id\"] == run_id),\n",
    "                    \"total_flowcell_yield_Gbp\"\n",
    "                ] = total_yield\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"‚ö†Ô∏è Failed fetching stats for {run_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    logger.info(f\"‚úî Final DataFrame shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "df = fetch_bclconvert_runs_with_yield(\n",
    "    api_base=API_BASE,\n",
    "    api_token=API_TOKEN,\n",
    "    curr_ds=\"2025-08-20\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b7005d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99add5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab8112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a4b27e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://ica.illumina.com/ica/rest/api\"\n",
    "PROJECT_ID = \"7feb6619-714b-48f7-a7fd-75ad264f9c55\"\n",
    "API_KEY=\"04LlMKg4K0asFGREmIXhucZ3IV2Hinx\"\n",
    "\n",
    "curr_ds=\"2025-05-21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "270a40af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_download_url(api_key: str, project_id: str, file_id: str) -> str:\n",
    "    url = f\"{BASE_URL}/projects/{project_id}/data/{file_id}:createDownloadUrl\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/vnd.illumina.v3+json\",\n",
    "        \"X-API-Key\": api_key\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data='')\n",
    "    response.raise_for_status()\n",
    "    return response.json().get(\"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d490cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-08-26T13:49:36.196+0700\u001b[0m] {\u001b[34m3112584711.py:\u001b[0m17} INFO\u001b[0m - Fetching analyses for: 2025-05-21\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:49:36.198+0700\u001b[0m] {\u001b[34m3112584711.py:\u001b[0m27} INFO\u001b[0m - Requesting URL: https://ica.illumina.com/ica/rest/api/projects/7feb6619-714b-48f7-a7fd-75ad264f9c55/analyses?pageSize=100&pageOffset=0&sort=reference%20desc\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:49:38.013+0700\u001b[0m] {\u001b[34m3112584711.py:\u001b[0m33} INFO\u001b[0m - Fetched 100 analyses (offset 0)\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:49:38.014+0700\u001b[0m] {\u001b[34m3112584711.py:\u001b[0m27} INFO\u001b[0m - Requesting URL: https://ica.illumina.com/ica/rest/api/projects/7feb6619-714b-48f7-a7fd-75ad264f9c55/analyses?pageSize=100&pageOffset=100&sort=reference%20desc\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:49:41.320+0700\u001b[0m] {\u001b[34m3112584711.py:\u001b[0m33} INFO\u001b[0m - Fetched 100 analyses (offset 100)\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:49:41.321+0700\u001b[0m] {\u001b[34m3112584711.py:\u001b[0m27} INFO\u001b[0m - Requesting URL: https://ica.illumina.com/ica/rest/api/projects/7feb6619-714b-48f7-a7fd-75ad264f9c55/analyses?pageSize=100&pageOffset=200&sort=reference%20desc\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:49:43.979+0700\u001b[0m] {\u001b[34m3112584711.py:\u001b[0m33} INFO\u001b[0m - Fetched 68 analyses (offset 200)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def parse_iso_utc(ts: str) -> datetime:\n",
    "    # e.g. \"2025-08-23T02:07:00Z\" -> aware datetime\n",
    "    return datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
    "\n",
    "analyses = []\n",
    "page_size = 100\n",
    "page_offset = 0\n",
    "\n",
    "HEADERS = {\n",
    "    \"accept\": \"application/vnd.illumina.v3+json\",\n",
    "    \"X-API-Key\": API_KEY\n",
    "}\n",
    "\n",
    "logger.info(f\"Fetching analyses for: {curr_ds}\")\n",
    "\n",
    "# Build UTC cutoff at start of day\n",
    "cutoff = datetime.strptime(curr_ds, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "\n",
    "while True:\n",
    "    url = (\n",
    "        f\"{BASE_URL}/projects/{PROJECT_ID}/analyses\"\n",
    "        f\"?pageSize={page_size}&pageOffset={page_offset}&sort=reference%20desc\"\n",
    "    )\n",
    "    logger.info(f\"Requesting URL: {url}\")\n",
    "    resp = requests.get(url, headers=HEADERS)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "\n",
    "    items = data.get(\"items\", [])\n",
    "    logger.info(f\"Fetched {len(items)} analyses (offset {page_offset})\")\n",
    "\n",
    "    # Keep only those with timeModified >= cutoff\n",
    "    for a in items:\n",
    "        tm = a.get(\"timeModified\")\n",
    "        if not tm:\n",
    "            continue\n",
    "        if parse_iso_utc(tm) >= cutoff:\n",
    "            analyses.append(a)\n",
    "\n",
    "    if len(items) < page_size:\n",
    "        break\n",
    "    page_offset += page_size\n",
    "\n",
    "if not analyses:\n",
    "    logger.info(\"No analyses found that meet timeModified cutoff.\")\n",
    "    latest_analyses = []\n",
    "else:\n",
    "    # Sort by timeModified (latest first)\n",
    "    latest_analyses = sorted(\n",
    "        analyses,\n",
    "        key=lambda a: parse_iso_utc(a[\"timeModified\"]),\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "# latest_analyses now has only items with timeModified >= curr_ds, newest first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01775fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-08-26T13:55:02.991+0700\u001b[0m] {\u001b[34m3034486776.py:\u001b[0m16} INFO\u001b[0m - Checking analysis reference: LP2508211-P1_b823b3_04e691-64321e29-bf95-42e0-9337-118876056dbf\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:55:05.464+0700\u001b[0m] {\u001b[34m3034486776.py:\u001b[0m16} INFO\u001b[0m - Checking analysis reference: LP2508131-P2_cd272a_8faf5b-c79b78eb-da2c-4fc8-9194-322e7dbc82ab\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:55:07.819+0700\u001b[0m] {\u001b[34m3034486776.py:\u001b[0m16} INFO\u001b[0m - Checking analysis reference: LP2508131-P1_62549f_199e16-f3b4d819-aabf-42a8-96fc-bd1d1cd93e63\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:55:10.079+0700\u001b[0m] {\u001b[34m3034486776.py:\u001b[0m16} INFO\u001b[0m - Checking analysis reference: LP2508121-P2_ae0335_382f7b-a0729fcd-d498-4cb7-84b4-caaa3c10f447\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:55:12.902+0700\u001b[0m] {\u001b[34m3034486776.py:\u001b[0m16} INFO\u001b[0m - Checking analysis reference: LP2508061-P1_9bbda7_194252-3e8f92e0-e3b9-458a-bde7-f85b6a840bdd\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:55:15.434+0700\u001b[0m] {\u001b[34m3034486776.py:\u001b[0m16} INFO\u001b[0m - Checking analysis reference: LP2508111-P1_17e6fc_3f190f-a4777f3b-7300-4b90-975e-b61f697c36eb\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:55:17.763+0700\u001b[0m] {\u001b[34m3034486776.py:\u001b[0m16} INFO\u001b[0m - Checking analysis reference: LP2508111-P2_62b3bf_900325-52f148ee-df6b-4cda-a890-e3611b5aabae\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:55:20.137+0700\u001b[0m] {\u001b[34m3034486776.py:\u001b[0m16} INFO\u001b[0m - Checking analysis reference: LP2508061-P2_4af418_6075cf-833705d3-889e-477d-9c9d-4a4c40fe20fc\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:55:22.688+0700\u001b[0m] {\u001b[34m3034486776.py:\u001b[0m16} INFO\u001b[0m - Checking analysis reference: LP2507231-P1_Redo_d2d85d_7c32ca-515ef2ac-42cd-48a4-90eb-c1aa8ca60e8c\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:55:25.266+0700\u001b[0m] {\u001b[34m3034486776.py:\u001b[0m16} INFO\u001b[0m - Checking analysis reference: LP2507252-P2_89b03e_bb27f4-e93f4536-33ac-4263-b514-5627bdad6cbb\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:55:27.893+0700\u001b[0m] {\u001b[34m3034486776.py:\u001b[0m16} INFO\u001b[0m - Checking analysis reference: LP2507281-P1_e270c0_fd7420-5e1a2e05-abd5-445d-ab78-b32cf00406fe\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:55:30.291+0700\u001b[0m] {\u001b[34m3034486776.py:\u001b[0m16} INFO\u001b[0m - Checking analysis reference: LP2507281-P2_42c312_e41e31-20012644-128b-47f8-950d-9a4ca64255d1\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:55:32.975+0700\u001b[0m] {\u001b[34m3034486776.py:\u001b[0m16} INFO\u001b[0m - Checking analysis reference: LP2507252-P1_01b3da_0e88db-b12d8dc4-de1b-48dc-a734-700a823f2d09\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:55:35.547+0700\u001b[0m] {\u001b[34m3034486776.py:\u001b[0m16} INFO\u001b[0m - Checking analysis reference: LP2507251-P2_e32aa7_3aecb8-de2cf6c7-6a2c-47ed-908a-bc5dc7d91ac3\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:55:38.109+0700\u001b[0m] {\u001b[34m3034486776.py:\u001b[0m16} INFO\u001b[0m - Checking analysis reference: LP2507231-P2_63c87f_6acdff-2a1b8314-890a-40d9-a3e4-20dbcf5408a0\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:55:40.612+0700\u001b[0m] {\u001b[34m3034486776.py:\u001b[0m16} INFO\u001b[0m - Checking analysis reference: LP2504281-P1_Redo_a4a8f8_212264-0e4fab24-a304-499b-8964-5a46c2f2df5e\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:55:43.183+0700\u001b[0m] {\u001b[34m3034486776.py:\u001b[0m16} INFO\u001b[0m - Checking analysis reference: LP2507231-P2_83a335_d124d5-f66f45ab-a196-455a-800a-b084df5333cf\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:55:44.285+0700\u001b[0m] {\u001b[34m3034486776.py:\u001b[0m16} INFO\u001b[0m - Checking analysis reference: LP2507251-P1_615d74_648ff1-41b1fc11-c1ee-4b6a-af47-922b83afdf49\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:55:46.732+0700\u001b[0m] {\u001b[34m3034486776.py:\u001b[0m16} INFO\u001b[0m - Checking analysis reference: LP2505151-P2-Redo_a59329_84d18b-47301902-e39c-440f-85a0-9629fe671502\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:55:49.030+0700\u001b[0m] {\u001b[34m3034486776.py:\u001b[0m16} INFO\u001b[0m - Checking analysis reference: LP2505151-P1-Redo_74bab2_ac0c72-e13346ca-66cc-472c-b9e9-7cc7588d27f5\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:55:51.497+0700\u001b[0m] {\u001b[34m3034486776.py:\u001b[0m16} INFO\u001b[0m - Checking analysis reference: LP2505201-P2_1cf7fb_4e7c14-bd572f87-2046-45c2-b1e8-411ce7057242\u001b[0m\n",
      "[\u001b[34m2025-08-26T13:55:54.012+0700\u001b[0m] {\u001b[34m3034486776.py:\u001b[0m16} INFO\u001b[0m - Checking analysis reference: LP2505201-P1_51d305_68db2a-e671b25a-7109-44e6-bd26-25c5d69d554b\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "def create_download_url(api_key: str, project_id: str, file_id: str, BASE_URL:str) -> str:\n",
    "    url = f\"{BASE_URL}/projects/{project_id}/data/{file_id}:createDownloadUrl\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/vnd.illumina.v3+json\",\n",
    "        \"X-API-Key\": api_key\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data='')\n",
    "    response.raise_for_status()\n",
    "    return response.json().get(\"url\")\n",
    "\n",
    "for analysis in latest_analyses:\n",
    "    reference = analysis.get(\"reference\")\n",
    "    logger.info(f\"Checking analysis reference: {reference}\")\n",
    "    if not reference:\n",
    "        continue\n",
    "\n",
    "    match = re.search(r\"(LP[-_]?\\d{7}(?:-P\\d)?(?:[-_](?:rerun|redo))?)\", str(reference), re.IGNORECASE)\n",
    "    if not match:\n",
    "        logger.warning(f\"Could not extract id_library from {reference}\")\n",
    "        continue\n",
    "    id_library = match.group(1)\n",
    "\n",
    "    # --- Handle Demultiplex_Stats.csv ---\n",
    "    demux_file_path = f\"/ilmn-analyses/{reference}/output/Reports/Demultiplex_Stats.csv\"\n",
    "    demux_encoded = urllib.parse.quote(demux_file_path)\n",
    "\n",
    "    demux_query = (\n",
    "        f\"{BASE_URL}/projects/{PROJECT_ID}/data\"\n",
    "        f\"?filePath={demux_encoded}\"\n",
    "        f\"&filenameMatchMode=EXACT\"\n",
    "        f\"&filePathMatchMode=STARTS_WITH_CASE_INSENSITIVE\"\n",
    "        f\"&status=AVAILABLE&type=FILE\"\n",
    "    )\n",
    "\n",
    "    demux_response = requests.get(demux_query, headers=HEADERS)\n",
    "    demux_response.raise_for_status()\n",
    "    demux_items = demux_response.json().get(\"items\", [])\n",
    "\n",
    "    if demux_items:\n",
    "        file_id = demux_items[0][\"data\"][\"id\"]\n",
    "        download_url = create_download_url(API_KEY, PROJECT_ID, file_id, BASE_URL)\n",
    "        response = requests.get(download_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # === Add id_library column ===\n",
    "        csv_buf = io.StringIO(response.content.decode(\"utf-8\"))\n",
    "        df = pd.read_csv(csv_buf)\n",
    "        df[\"id_library\"] = id_library  # append new column\n",
    "\n",
    "        # Convert back to CSV bytes\n",
    "        csv_bytes = df.to_csv(index=False).encode(\"utf-8\")\n",
    "\n",
    "        # Upload to S3\n",
    "        s3_key = f\"{object_path_prefix}/{reference}/{id_library}_Demultiplex_Stats.csv\"\n",
    "        s3.load_bytes(\n",
    "            bytes_data=csv_bytes,\n",
    "            key=s3_key,\n",
    "            bucket_name=bucket_name,\n",
    "            replace=True\n",
    "        )\n",
    "        logger.info(f\"Uploaded Demultiplex_Stats with id_library to: s3://{bucket_name}/{s3_key}\")\n",
    "    else:\n",
    "        logger.info(f\"Demultiplex_Stats.csv not found for {reference}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d491cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5940c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LP2505201-P1'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Quality_Metrics.csv ---\n",
    "quality_file_path = f\"/ilmn-analyses/{reference}/output/Reports/Quality_Metrics.csv\"\n",
    "quality_encoded = urllib.parse.quote(quality_file_path)\n",
    "\n",
    "quality_query = (\n",
    "    f\"{BASE_URL}/projects/{PROJECT_ID}/data\"\n",
    "    f\"?filePath={quality_encoded}\"\n",
    "    f\"&filenameMatchMode=EXACT\"\n",
    "    f\"&filePathMatchMode=STARTS_WITH_CASE_INSENSITIVE\"\n",
    "    f\"&status=AVAILABLE&type=FILE\"\n",
    ")\n",
    "\n",
    "quality_response = requests.get(quality_query, headers=HEADERS)\n",
    "quality_response.raise_for_status()\n",
    "quality_items = quality_response.json().get(\"items\", [])\n",
    "\n",
    "if quality_items:\n",
    "    file_id = quality_items[0][\"data\"][\"id\"]\n",
    "    # NOTE: your create_download_url requires BASE_URL param\n",
    "    download_url = create_download_url(API_KEY, PROJECT_ID, file_id, BASE_URL)\n",
    "    response = requests.get(download_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Add id_library column\n",
    "    q_csv_buf = io.StringIO(response.content.decode(\"utf-8\"))\n",
    "    q_df = pd.read_csv(q_csv_buf)\n",
    "    q_df[\"id_library\"] = id_library  # appended as last column\n",
    "\n",
    "    # Convert back to CSV bytes\n",
    "    q_csv_bytes = q_df.to_csv(index=False).encode(\"utf-8\")\n",
    "\n",
    "    # Final S3 key format: illumina/qs/{file_id}/Quality_Metrics.csv\n",
    "    qs_s3_key = f\"illumina/qs/{file_id}/Quality_Metrics.csv\"\n",
    "    s3.load_bytes(\n",
    "        bytes_data=q_csv_bytes,\n",
    "        key=qs_s3_key,\n",
    "        bucket_name=\"bgsi-data-dwh-bronze\",\n",
    "        replace=True\n",
    "    )\n",
    "    logger.info(f\"Uploaded Quality_Metrics (with id_library) to: s3://bgsi-data-dwh-bronze/{qs_s3_key}\")\n",
    "else:\n",
    "    logger.info(f\"Quality_Metrics.csv not found for {reference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206a4379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6079bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import urllib.parse\n",
    "from typing import Dict, Any, List\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n",
    "def sync_ica_qc_to_s3(\n",
    "    *,                     # \"YYYY-MM-DD\"\n",
    "    API_KEY: str,\n",
    "    PROJECT_ID: str,\n",
    "    BASE_URL: str,                    # e.g. \"https://ica.illumina.com/ica/rest/api\"\n",
    "    bucket_name: str,                 # destination bucket for Demux\n",
    "    object_path_prefix: str,          # prefix for Demux (e.g. \"illumina/demux\")\n",
    "    s3_hook,                          # Airflow S3Hook instance\n",
    "    logger,                            # Airflow logger\n",
    "    **kwargs\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch ICA analyses (timeModified >= curr_ds), pull Demultiplex_Stats.csv and Quality_Metrics.csv,\n",
    "    append id_library column, and upload to S3.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "          \"analyses_considered\": int,\n",
    "          \"analyses_processed\": int,\n",
    "          \"demux_uploaded\": int,\n",
    "          \"quality_uploaded\": int,\n",
    "          \"per_analysis\": [{ \"reference\": str, \"demux\": \"uploaded|not_found|error\", \"quality\": \"uploaded|not_found|error\" }, ...]\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    def create_download_url(api_key: str, project_id: str, file_id: str, base_url: str) -> str:\n",
    "        url = f\"{base_url}/projects/{project_id}/data/{file_id}:createDownloadUrl\"\n",
    "        headers = {\n",
    "            \"accept\": \"application/vnd.illumina.v3+json\",\n",
    "            \"X-API-Key\": api_key\n",
    "        }\n",
    "        r = requests.post(url, headers=headers, data=\"\")\n",
    "        r.raise_for_status()\n",
    "        return r.json().get(\"url\")\n",
    "\n",
    "    def parse_iso_utc(ts: str) -> datetime:\n",
    "        return datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
    "\n",
    "    LP_REGEX = re.compile(r\"(LP[-_]?\\d{7}(?:-P\\d)?(?:[-_](?:rerun|redo))?)\", re.IGNORECASE)\n",
    "\n",
    "    def lookup_file_by_path(file_path: str) -> List[dict]:\n",
    "        encoded = urllib.parse.quote(file_path)\n",
    "        q = (\n",
    "            f\"{BASE_URL}/projects/{PROJECT_ID}/data\"\n",
    "            f\"?filePath={encoded}\"\n",
    "            f\"&filenameMatchMode=EXACT\"\n",
    "            f\"&filePathMatchMode=STARTS_WITH_CASE_INSENSITIVE\"\n",
    "            f\"&status=AVAILABLE&type=FILE\"\n",
    "        )\n",
    "        rr = requests.get(q, headers=HEADERS)\n",
    "        rr.raise_for_status()\n",
    "        return rr.json().get(\"items\", [])\n",
    "\n",
    "    def download_csv_bytes(file_id: str) -> bytes:\n",
    "        url = create_download_url(API_KEY, PROJECT_ID, file_id, BASE_URL)\n",
    "        r = requests.get(url)\n",
    "        r.raise_for_status()\n",
    "        return r.content\n",
    "\n",
    "    def add_id_library(csv_bytes: bytes, id_library: str) -> bytes:\n",
    "        buf = io.StringIO(csv_bytes.decode(\"utf-8\"))\n",
    "        df = pd.read_csv(buf)\n",
    "        df[\"id_library\"] = id_library\n",
    "        return df.to_csv(index=False).encode(\"utf-8\")\n",
    "\n",
    "    # ---------- fetch analyses (filtered by timeModified >= cutoff) ----------\n",
    "    HEADERS = {\n",
    "        \"accept\": \"application/vnd.illumina.v3+json\",\n",
    "        \"X-API-Key\": API_KEY\n",
    "    }\n",
    "    cutoff = datetime.strptime(kwargs.get(\"ds\"), \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "\n",
    "    analyses: List[dict] = []\n",
    "    page_size = 100\n",
    "    page_offset = 0\n",
    "\n",
    "    logger.info(f\"[ICA] Fetching analyses updated on/after {curr_ds} (UTC start-of-day cutoff)\")\n",
    "\n",
    "    while True:\n",
    "        url = (\n",
    "            f\"{BASE_URL}/projects/{PROJECT_ID}/analyses\"\n",
    "            f\"?pageSize={page_size}&pageOffset={page_offset}&sort=reference%20desc\"\n",
    "        )\n",
    "        logger.info(f\"[ICA] GET {url}\")\n",
    "        resp = requests.get(url, headers=HEADERS)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        items = data.get(\"items\", [])\n",
    "\n",
    "        logger.info(f\"[ICA] Page offset {page_offset}: {len(items)} analyses\")\n",
    "\n",
    "        for a in items:\n",
    "            tm = a.get(\"timeModified\")\n",
    "            if tm and parse_iso_utc(tm) >= cutoff:\n",
    "                analyses.append(a)\n",
    "\n",
    "        if len(items) < page_size:\n",
    "            break\n",
    "        page_offset += page_size\n",
    "\n",
    "    latest_analyses = sorted(\n",
    "        analyses, key=lambda a: parse_iso_utc(a[\"timeModified\"]), reverse=True\n",
    "    ) if analyses else []\n",
    "\n",
    "    # ---------- process each analysis ----------\n",
    "    summary = {\n",
    "        \"analyses_considered\": len(latest_analyses),\n",
    "        \"analyses_processed\": 0,\n",
    "        \"demux_uploaded\": 0,\n",
    "        \"quality_uploaded\": 0,\n",
    "        \"per_analysis\": []\n",
    "    }\n",
    "\n",
    "    for analysis in latest_analyses:\n",
    "        reference = analysis.get(\"reference\")\n",
    "        if not reference:\n",
    "            continue\n",
    "\n",
    "        status_row = {\"reference\": reference, \"demux\": \"not_found\", \"quality\": \"not_found\"}\n",
    "\n",
    "        m = LP_REGEX.search(str(reference))\n",
    "        if not m:\n",
    "            logger.warning(f\"[ICA] Could not extract id_library from reference: {reference}\")\n",
    "            summary[\"per_analysis\"].append(status_row)\n",
    "            continue\n",
    "        id_library = m.group(1)\n",
    "\n",
    "        # --- Demultiplex_Stats.csv ---\n",
    "        try:\n",
    "            demux_path = f\"/ilmn-analyses/{reference}/output/Reports/Demultiplex_Stats.csv\"\n",
    "            demux_items = lookup_file_by_path(demux_path)\n",
    "\n",
    "            if demux_items:\n",
    "                demux_file_id = demux_items[0][\"data\"][\"id\"]\n",
    "                raw_bytes = download_csv_bytes(demux_file_id)\n",
    "                out_bytes = add_id_library(raw_bytes, id_library)\n",
    "\n",
    "                demux_s3_key = f\"{object_path_prefix}/{reference}/{id_library}_Demultiplex_Stats.csv\"\n",
    "                s3_hook.load_bytes(\n",
    "                    bytes_data=out_bytes,\n",
    "                    key=demux_s3_key,\n",
    "                    bucket_name=bucket_name,\n",
    "                    replace=True\n",
    "                )\n",
    "                logger.info(f\"[S3] Uploaded Demultiplex_Stats ‚Üí s3://{bucket_name}/{demux_s3_key}\")\n",
    "                status_row[\"demux\"] = \"uploaded\"\n",
    "                summary[\"demux_uploaded\"] += 1\n",
    "            else:\n",
    "                logger.info(f\"[ICA] Demultiplex_Stats.csv not found for {reference}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[ICA] Error processing Demultiplex_Stats for {reference}: {e}\", exc_info=True)\n",
    "            status_row[\"demux\"] = \"error\"\n",
    "\n",
    "        # --- Quality_Metrics.csv ---\n",
    "        try:\n",
    "            quality_path = f\"/ilmn-analyses/{reference}/output/Reports/Quality_Metrics.csv\"\n",
    "            quality_items = lookup_file_by_path(quality_path)\n",
    "\n",
    "            if quality_items:\n",
    "                quality_file_id = quality_items[0][\"data\"][\"id\"]\n",
    "                raw_bytes = download_csv_bytes(quality_file_id)\n",
    "                out_bytes = add_id_library(raw_bytes, id_library)\n",
    "\n",
    "                qs_s3_key = f\"illumina/qs/{quality_file_id}/Quality_Metrics.csv\"\n",
    "                s3_hook.load_bytes(\n",
    "                    bytes_data=out_bytes,\n",
    "                    key=qs_s3_key,\n",
    "                    bucket_name=\"bgsi-data-dwh-bronze\",\n",
    "                    replace=True\n",
    "                )\n",
    "                logger.info(f\"[S3] Uploaded Quality_Metrics ‚Üí s3://bgsi-data-dwh-bronze/{qs_s3_key}\")\n",
    "                status_row[\"quality\"] = \"uploaded\"\n",
    "                summary[\"quality_uploaded\"] += 1\n",
    "            else:\n",
    "                logger.info(f\"[ICA] Quality_Metrics.csv not found for {reference}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[ICA] Error processing Quality_Metrics for {reference}: {e}\", exc_info=True)\n",
    "            status_row[\"quality\"] = \"error\"\n",
    "\n",
    "        summary[\"analyses_processed\"] += 1\n",
    "        summary[\"per_analysis\"].append(status_row)\n",
    "\n",
    "    logger.info(\n",
    "        f\"[DONE] considered={summary['analyses_considered']}, \"\n",
    "        f\"processed={summary['analyses_processed']}, \"\n",
    "        f\"demux_uploaded={summary['demux_uploaded']}, \"\n",
    "        f\"quality_uploaded={summary['quality_uploaded']}\"\n",
    "    )\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab93f73e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
